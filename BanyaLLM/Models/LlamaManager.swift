//
//  LlamaManager.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

@MainActor
class LlamaManager: ObservableObject {
    @Published var isModelLoaded: Bool = false
    @Published var loadingProgress: String = ""
    
    private var llamaContext: LlamaContext?
    private let modelFilename = "llama31-banyaa-q4_k_m.gguf"
    
    // Llama 3.1 System Prompt (ëŒ€í™” í’ˆì§ˆ í–¥ìƒ)
    private let systemPrompt = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ëŠ¥ìˆ™í•œ í•œêµ­ì–´ ëŒ€í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ë©°, ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ë©°, ë¶ˆí•„ìš”í•œ ì„œë¡ ì€ í”¼í•©ë‹ˆë‹¤.
ëª¨ë¥´ëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” ì†”ì§í•˜ê²Œ "ì£„ì†¡í•˜ì§€ë§Œ ê·¸ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
"""
    
    nonisolated init() {
        // ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ í˜¸ì¶œ
    }
    
    func initialize() {
        Task {
            await loadModel()
        }
    }
    
    // MARK: - Llama 3.1 Chat Template
    
    /// Llama 3.1 ê³µì‹ Chat Template ì ìš©
    /// - Parameter userMessage: ì‚¬ìš©ì ë©”ì‹œì§€
    /// - Returns: í¬ë§·ëœ ì „ì²´ í”„ë¡¬í”„íŠ¸
    private func formatChatPrompt(userMessage: String) -> String {
        let bos = "<|begin_of_text|>"
        let startHeader = "<|start_header_id|>"
        let endHeader = "<|end_header_id|>"
        let eot = "<|eot_id|>"
        
        let formattedPrompt = """
\(bos)\(startHeader)system\(endHeader)

\(systemPrompt)\(eot)\(startHeader)user\(endHeader)

\(userMessage)\(eot)\(startHeader)assistant\(endHeader)

"""
        
        return formattedPrompt
    }
    
    func loadModel() async {
        do {
            // 1. ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
            if let savedPath = UserDefaults.standard.string(forKey: "selectedModelPath") {
                print("ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ ë°œê²¬: \(savedPath)")
                
                if FileManager.default.fileExists(atPath: savedPath) {
                    print("âœ… ì €ì¥ëœ ê²½ë¡œì— íŒŒì¼ ì¡´ì¬ - ìë™ ë¡œë“œ ì‹œë„")
                    let success = await loadModelFromPath(savedPath)
                    
                    if success {
                        print("âœ… ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì„±ê³µ")
                        return
                    } else {
                        print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ê²½ë¡œ ì œê±°")
                        UserDefaults.standard.removeObject(forKey: "selectedModelPath")
                    }
                } else {
                    print("âš ï¸ ì €ì¥ëœ ê²½ë¡œì— íŒŒì¼ ì—†ìŒ - ê²½ë¡œ ì œê±°")
                    UserDefaults.standard.removeObject(forKey: "selectedModelPath")
                }
            }
            
            // 2. ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
            print("ğŸ” ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ê²€ìƒ‰")
            let modelPath = try getModelPath()
            await loadModelFromPath(modelPath)
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”"
            print("â„¹ï¸ ëª¨ë¸ íŒŒì¼ ì„ íƒ í•„ìš”")
        }
    }
    
    @discardableResult
    func loadModelFromPath(_ path: String) async -> Bool {
        do {
            loadingProgress = "ëª¨ë¸ ë¡œë”© ì¤‘..."
            print("ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì‹œì‘: \(path)")
            
            // LlamaContext ìƒì„± ë° ì´ˆê¸°í™”
            llamaContext = LlamaContext(modelPath: path)
            try await llamaContext?.initialize()
            
            isModelLoaded = true
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
            print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
            
            // ì„±ê³µ ì‹œ ê²½ë¡œ ì €ì¥
            UserDefaults.standard.set(path, forKey: "selectedModelPath")
            print("ğŸ’¾ ëª¨ë¸ ê²½ë¡œ ì €ì¥: \(path)")
            
            return true
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error.localizedDescription)"
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
            
            return false
        }
    }
    
    private func getModelPath() throws -> String {
        // í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê°œë°œ ì¤‘)
        let projectPath = "/Volumes/Transcend/Projects/BanyaLLM/BanyaLLM/\(modelFilename)"
        if FileManager.default.fileExists(atPath: projectPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(projectPath)")
            return projectPath
        }
        
        // Documents ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let modelPath = documentsPath.appendingPathComponent(modelFilename).path
        
        if FileManager.default.fileExists(atPath: modelPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(modelPath)")
            return modelPath
        }
        
        // Bundleì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ë°°í¬ ì‹œ)
        if let path = Bundle.main.path(forResource: "llama31-banyaa-q4_k_m", ofType: "gguf") {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(path)")
            return path
        }
        
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. \(projectPath)")
        print("2. \(modelPath)")
        
        throw LlamaError.modelNotFound
    }
    
    func generate(prompt: String) async -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                #if targetEnvironment(simulator)
                // ì‹œë®¬ë ˆì´í„°: ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                let responses = [
                    "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” BanyaLLMì…ë‹ˆë‹¤.",
                    "\n\n",
                    "í˜„ì¬ ì‹œë®¬ë ˆì´í„°ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.",
                    "\n\n",
                    "ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ iPhoneì´ë‚˜ iPad ì‹¤ì œ ê¸°ê¸°ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”!",
                    "\n\n",
                    "ì§ˆë¬¸: \"\(prompt)\""
                ]
                
                for token in responses {
                    continuation.yield(token)
                    try? await Task.sleep(nanoseconds: 50_000_000)
                }
                continuation.finish()
                #else
                
                    guard let llamaContext = self.llamaContext else {
                        print("âŒ LlamaContextê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        continuation.yield("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
                        continuation.finish()
                        return
                    }
                    
                    print("ğŸ¯ LLM ìƒì„± ì‹œì‘")
                    
                    // Llama 3.1 Chat Template ì ìš©
                    let formattedPrompt = self.formatChatPrompt(userMessage: prompt)
                    
                    // LLM ì¶”ë¡  ì´ˆê¸°í™”
                    await llamaContext.completionInit(text: formattedPrompt)
                    
                    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
                    while await !llamaContext.isDone {
                        let token = await llamaContext.completionLoop()
                        
                        if !token.isEmpty {
                            continuation.yield(token)
                            // ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                            try? await Task.sleep(nanoseconds: 50_000_000)
                        }
                    }
                    
                    print("âœ… ìƒì„± ì™„ë£Œ")
                    
                    // ì¶”ë¡  ì™„ë£Œ í›„ ì •ë¦¬
                    await llamaContext.clear()
                    continuation.finish()
                #endif
            }
        }
    }
}

