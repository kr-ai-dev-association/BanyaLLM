//
//  LlamaManager.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

@MainActor
class LlamaManager: ObservableObject {
    @Published var isModelLoaded: Bool = false
    @Published var loadingProgress: String = ""
    
    private var llamaContext: LlamaContext?
    private let modelFilename = "llama31-banyaa-q4_k_m.gguf"
    
    nonisolated init() {
        // ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ í˜¸ì¶œ
    }
    
    func initialize() {
        Task {
            await loadModel()
        }
    }
    
    func loadModel() async {
        do {
            // ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
            if let savedPath = UserDefaults.standard.string(forKey: "selectedModelPath"),
               FileManager.default.fileExists(atPath: savedPath) {
                print("ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: \(savedPath)")
                await loadModelFromPath(savedPath)
                return
            }
            
            // ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
            let modelPath = try getModelPath()
            await loadModelFromPath(modelPath)
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”"
            print("â„¹ï¸ ëª¨ë¸ íŒŒì¼ ì„ íƒ í•„ìš”")
        }
    }
    
    func loadModelFromPath(_ path: String) async {
        do {
            loadingProgress = "ëª¨ë¸ ë¡œë”© ì¤‘..."
            print("ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì‹œì‘: \(path)")
            
            // LlamaContext ìƒì„± ë° ì´ˆê¸°í™”
            llamaContext = LlamaContext(modelPath: path)
            try await llamaContext?.initialize()
            
            isModelLoaded = true
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
            print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
            
            // ì„±ê³µ ì‹œ ê²½ë¡œ ì €ì¥
            UserDefaults.standard.set(path, forKey: "selectedModelPath")
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error.localizedDescription)"
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
        }
    }
    
    private func getModelPath() throws -> String {
        // í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê°œë°œ ì¤‘)
        let projectPath = "/Volumes/Transcend/Projects/BanyaLLM/BanyaLLM/\(modelFilename)"
        if FileManager.default.fileExists(atPath: projectPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(projectPath)")
            return projectPath
        }
        
        // Documents ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let modelPath = documentsPath.appendingPathComponent(modelFilename).path
        
        if FileManager.default.fileExists(atPath: modelPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(modelPath)")
            return modelPath
        }
        
        // Bundleì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ë°°í¬ ì‹œ)
        if let path = Bundle.main.path(forResource: "llama31-banyaa-q4_k_m", ofType: "gguf") {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(path)")
            return path
        }
        
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. \(projectPath)")
        print("2. \(modelPath)")
        
        throw LlamaError.modelNotFound
    }
    
    func generate(prompt: String) async -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                #if targetEnvironment(simulator)
                // ì‹œë®¬ë ˆì´í„°: ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                let responses = [
                    "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” BanyaLLMì…ë‹ˆë‹¤.",
                    "\n\n",
                    "í˜„ì¬ ì‹œë®¬ë ˆì´í„°ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.",
                    "\n\n",
                    "ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ iPhoneì´ë‚˜ iPad ì‹¤ì œ ê¸°ê¸°ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”!",
                    "\n\n",
                    "ì§ˆë¬¸: \"\(prompt)\""
                ]
                
                for token in responses {
                    continuation.yield(token)
                    try? await Task.sleep(nanoseconds: 50_000_000)
                }
                continuation.finish()
                #else
                
                guard let llamaContext = self.llamaContext else {
                    print("âŒ LlamaContextê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    continuation.yield("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
                    continuation.finish()
                    return
                }
                
                // LLM ì¶”ë¡  ì´ˆê¸°í™”
                await llamaContext.completionInit(text: prompt)
                
                // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
                while await !llamaContext.isDone {
                    let token = await llamaContext.completionLoop()
                    
                    if !token.isEmpty {
                        continuation.yield(token)
                        // ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                        try? await Task.sleep(nanoseconds: 50_000_000) // 0.05ì´ˆ
                    }
                }
                
                // ì¶”ë¡  ì™„ë£Œ í›„ ì •ë¦¬
                await llamaContext.clear()
                continuation.finish()
                #endif
            }
        }
    }
}

