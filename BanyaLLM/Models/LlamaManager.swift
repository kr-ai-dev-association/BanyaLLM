//
//  LlamaManager.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

@MainActor
class LlamaManager: ObservableObject {
    @Published var isModelLoaded: Bool = false
    @Published var loadingProgress: String = ""
    
    private var llamaContext: LlamaContext?
    private let modelFilename = "llama31-banyaa-q4_k_m.gguf"
    
    // Llama 3.1 System Prompt (ëŒ€í™” í’ˆì§ˆ í–¥ìƒ)
    private let systemPrompt = """
ë‹¹ì‹ ì€ ë§¤ìš° ê°„ê²°í•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ ëŒ€í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ë©°, ì§ˆë¬¸ì— í•µì‹¬ë§Œ 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
ì ˆëŒ€ ë°˜ë³µí•˜ì§€ ì•Šê³ , ê°™ì€ ë‚´ìš©ì„ ë‘ ë²ˆ ë§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì¥í™©í•œ ì„¤ëª…ì´ë‚˜ ë¶ˆí•„ìš”í•œ ì˜ˆì‹œë¥¼ í”¼í•˜ê³ , í•µì‹¬ë§Œ ê°„ë‹¨íˆ ë§í•©ë‹ˆë‹¤.
ëª¨ë¥´ëŠ” ì •ë³´ëŠ” "ì£„ì†¡í•˜ì§€ë§Œ ê·¸ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
"""
    
    nonisolated init() {
        // ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ í˜¸ì¶œ
    }
    
    func initialize() {
        Task {
            await loadModel()
        }
    }
    
    // MARK: - Llama 3.1 Chat Template
    
    /// Llama 3.1 ê³µì‹ Chat Template ì ìš©
    /// - Parameter userMessage: ì‚¬ìš©ì ë©”ì‹œì§€
    /// - Returns: í¬ë§·ëœ ì „ì²´ í”„ë¡¬í”„íŠ¸
    private func formatChatPrompt(userMessage: String) -> String {
        let bos = "<|begin_of_text|>"
        let startHeader = "<|start_header_id|>"
        let endHeader = "<|end_header_id|>"
        let eot = "<|eot_id|>"
        
        let formattedPrompt = """
\(bos)\(startHeader)system\(endHeader)

\(systemPrompt)\(eot)\(startHeader)user\(endHeader)

\(userMessage)\(eot)\(startHeader)assistant\(endHeader)

"""
        
        return formattedPrompt
    }
    
    func loadModel() async {
        do {
            // 1. ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
            if let savedPath = UserDefaults.standard.string(forKey: "selectedModelPath") {
                print("ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ ë°œê²¬: \(savedPath)")
                
                if FileManager.default.fileExists(atPath: savedPath) {
                    print("âœ… ì €ì¥ëœ ê²½ë¡œì— íŒŒì¼ ì¡´ì¬ - ìë™ ë¡œë“œ ì‹œë„")
                    let success = await loadModelFromPath(savedPath)
                    
                    if success {
                        print("âœ… ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì„±ê³µ")
                        return
                    } else {
                        print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ê²½ë¡œ ì œê±°")
                        UserDefaults.standard.removeObject(forKey: "selectedModelPath")
                    }
                } else {
                    print("âš ï¸ ì €ì¥ëœ ê²½ë¡œì— íŒŒì¼ ì—†ìŒ - ê²½ë¡œ ì œê±°")
                    UserDefaults.standard.removeObject(forKey: "selectedModelPath")
                }
            }
            
            // 2. ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
            print("ğŸ” ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ê²€ìƒ‰")
            let modelPath = try getModelPath()
            await loadModelFromPath(modelPath)
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”"
            print("â„¹ï¸ ëª¨ë¸ íŒŒì¼ ì„ íƒ í•„ìš”")
        }
    }
    
    @discardableResult
    func loadModelFromPath(_ path: String) async -> Bool {
        do {
            loadingProgress = "ëª¨ë¸ ë¡œë”© ì¤‘..."
            print("ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì‹œì‘: \(path)")
            
            // LlamaContext ìƒì„± ë° ì´ˆê¸°í™”
            llamaContext = LlamaContext(modelPath: path)
            try await llamaContext?.initialize()
            
            isModelLoaded = true
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
            print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
            
            // ì„±ê³µ ì‹œ ê²½ë¡œ ì €ì¥
            UserDefaults.standard.set(path, forKey: "selectedModelPath")
            print("ğŸ’¾ ëª¨ë¸ ê²½ë¡œ ì €ì¥: \(path)")
            
            return true
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error.localizedDescription)"
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
            
            return false
        }
    }
    
    private func getModelPath() throws -> String {
        // í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê°œë°œ ì¤‘)
        let projectPath = "/Volumes/Transcend/Projects/BanyaLLM/BanyaLLM/\(modelFilename)"
        if FileManager.default.fileExists(atPath: projectPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(projectPath)")
            return projectPath
        }
        
        // Documents ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let modelPath = documentsPath.appendingPathComponent(modelFilename).path
        
        if FileManager.default.fileExists(atPath: modelPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(modelPath)")
            return modelPath
        }
        
        // Bundleì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ë°°í¬ ì‹œ)
        if let path = Bundle.main.path(forResource: "llama31-banyaa-q4_k_m", ofType: "gguf") {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(path)")
            return path
        }
        
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. \(projectPath)")
        print("2. \(modelPath)")
        
        throw LlamaError.modelNotFound
    }
    
    func generate(prompt: String) async -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                #if targetEnvironment(simulator)
                // ì‹œë®¬ë ˆì´í„°: ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                let responses = [
                    "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” BanyaLLMì…ë‹ˆë‹¤.",
                    "\n\n",
                    "í˜„ì¬ ì‹œë®¬ë ˆì´í„°ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.",
                    "\n\n",
                    "ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ iPhoneì´ë‚˜ iPad ì‹¤ì œ ê¸°ê¸°ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”!",
                    "\n\n",
                    "ì§ˆë¬¸: \"\(prompt)\""
                ]
                
                for token in responses {
                    continuation.yield(token)
                    try? await Task.sleep(nanoseconds: 50_000_000)
                }
                continuation.finish()
                #else
                
                    guard let llamaContext = self.llamaContext else {
                        print("âŒ LlamaContextê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        continuation.yield("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
                        continuation.finish()
                        return
                    }
                    
                    // Llama 3.1 Chat Template ì ìš©
                    let formattedPrompt = self.formatChatPrompt(userMessage: prompt)
                    
                    // LLM ì¶”ë¡  ì´ˆê¸°í™”
                    await llamaContext.completionInit(text: formattedPrompt)
                    
                    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ê°•í™”ëœ íŠ¹ìˆ˜ í† í° í•„í„°ë§)
                    var accumulatedRaw = ""
                    var previousCleanedLength = 0
                    let specialTokenPatterns = [
                        "<|begin_of_text|>",
                        "<|end_of_text|>",
                        "<|start_header_id|>",
                        "<|end_header_id|>",
                        "<|eot_id|>",
                        "<|eom_id|>",
                        "<|python_tag|>",
                        "<|finetune_right_pad_id|>"
                    ]
                    
                    func filterSpecialTokens(_ text: String) -> String {
                        var cleaned = text
                        
                        // 1. ì™„ì „í•œ íŠ¹ìˆ˜ í† í° íŒ¨í„´ ì œê±° (ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ì¤‘ì²© íŒ¨í„´ë„ ì²˜ë¦¬)
                        var previousLength = 0
                        while cleaned.count != previousLength {
                            previousLength = cleaned.count
                            for pattern in specialTokenPatterns {
                                cleaned = cleaned.replacingOccurrences(of: pattern, with: "")
                            }
                        }
                        
                        // 2. reserved_special_token íŒ¨í„´ ì œê±°
                        if let regex = try? NSRegularExpression(pattern: "<\\|reserved_special_token_\\d+\\|>", options: []) {
                            let range = NSRange(cleaned.startIndex..., in: cleaned)
                            cleaned = regex.stringByReplacingMatches(
                                in: cleaned,
                                options: [],
                                range: range,
                                withTemplate: ""
                            )
                        }
                        
                        // 3. ë¶€ë¶„ íŠ¹ìˆ˜ í† í° íŒ¨í„´ ì œê±° (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
                        // ìµœê·¼ 30ì ë‚´ì—ì„œ "<|" + "|>" ì¡°í•© ì°¾ê¸°
                        let windowSize = 30
                        if cleaned.count >= windowSize {
                            let recentText = String(cleaned.suffix(windowSize))
                            // "<|"ë¡œ ì‹œì‘í•˜ê³  "|>"ë¡œ ëë‚˜ëŠ” íŒ¨í„´ ì°¾ê¸°
                            if let startIndex = recentText.lastIndex(of: "<"),
                               let pipeAfter = recentText.index(startIndex, offsetBy: 1, limitedBy: recentText.endIndex),
                               pipeAfter < recentText.endIndex && recentText[pipeAfter] == "|",
                               let endIndex = recentText.range(of: "|>", range: pipeAfter..<recentText.endIndex)?.upperBound {
                                // íŠ¹ìˆ˜ í† í° íŒ¨í„´ ë°œê²¬: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì œê±°
                                let globalStartOffset = cleaned.count - windowSize + recentText.distance(from: recentText.startIndex, to: startIndex)
                                let globalEndOffset = cleaned.count - windowSize + recentText.distance(from: recentText.startIndex, to: endIndex)
                                
                                let globalStart = cleaned.index(cleaned.startIndex, offsetBy: globalStartOffset)
                                let globalEnd = cleaned.index(cleaned.startIndex, offsetBy: globalEndOffset)
                                cleaned = String(cleaned[..<globalStart]) + String(cleaned[globalEnd...])
                            }
                        }
                        
                        return cleaned
                    }
                    
                    while await !llamaContext.isDone {
                        let token = await llamaContext.completionLoop()
                        
                        if !token.isEmpty {
                            accumulatedRaw += token
                            
                            // ê°•í™”ëœ íŠ¹ìˆ˜ í† í° í•„í„°ë§
                            var cleanedText = filterSpecialTokens(accumulatedRaw)
                            
                            // ì´ì „ì— ì¶œë ¥í•œ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ìƒˆë¡œìš´ ë¶€ë¶„ë§Œ ì¶œë ¥
                            if cleanedText.count > previousCleanedLength {
                                let newContent = String(cleanedText.dropFirst(previousCleanedLength))
                                if !newContent.isEmpty {
                                    continuation.yield(newContent)
                                    previousCleanedLength = cleanedText.count
                                }
                            } else if cleanedText.count < previousCleanedLength {
                                // í•„í„°ë§ìœ¼ë¡œ ì¸í•´ í…ìŠ¤íŠ¸ê°€ ì¤„ì–´ë“  ê²½ìš° (íŠ¹ìˆ˜ í† í° ì œê±°ë¨)
                                previousCleanedLength = cleanedText.count
                            }
                            
                            // ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                            try? await Task.sleep(nanoseconds: 50_000_000)
                        }
                    }
                    
                    // ì¶”ë¡  ì™„ë£Œ í›„ ì •ë¦¬
                    await llamaContext.clear()
                    continuation.finish()
                #endif
            }
        }
    }
}

