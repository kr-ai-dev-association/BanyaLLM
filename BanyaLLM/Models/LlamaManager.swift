//
//  LlamaManager.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

@MainActor
class LlamaManager: ObservableObject {
    @Published var isModelLoaded: Bool = false
    @Published var loadingProgress: String = ""
    
    private var llamaContext: LlamaContext?
    private let modelFilename = "llama31-banyaa-q4_k_m.gguf"
    
    nonisolated init() {
        // ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ í˜¸ì¶œ
    }
    
    func initialize() {
        Task {
            await loadModel()
        }
    }
    
    func loadModel() async {
        do {
            // ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            let modelPath = try getModelPath()
            
            loadingProgress = "ëª¨ë¸ ë¡œë”© ì¤‘..."
            
            // LlamaContext ìƒì„± ë° ì´ˆê¸°í™”
            llamaContext = LlamaContext(modelPath: modelPath)
            try await llamaContext?.initialize()
            
            isModelLoaded = true
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
            print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: \(modelPath)")
            
        } catch {
            isModelLoaded = false
            loadingProgress = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error.localizedDescription)"
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
        }
    }
    
    private func getModelPath() throws -> String {
        // í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê°œë°œ ì¤‘)
        let projectPath = "/Volumes/Transcend/Projects/BanyaLLM/BanyaLLM/\(modelFilename)"
        if FileManager.default.fileExists(atPath: projectPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(projectPath)")
            return projectPath
        }
        
        // Documents ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let modelPath = documentsPath.appendingPathComponent(modelFilename).path
        
        if FileManager.default.fileExists(atPath: modelPath) {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(modelPath)")
            return modelPath
        }
        
        // Bundleì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ë°°í¬ ì‹œ)
        if let path = Bundle.main.path(forResource: "llama31-banyaa-q4_k_m", ofType: "gguf") {
            print("ğŸ“ ëª¨ë¸ ê²½ë¡œ: \(path)")
            return path
        }
        
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. \(projectPath)")
        print("2. \(modelPath)")
        
        throw LlamaError.modelNotFound
    }
    
    func generate(prompt: String) async -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                guard let llamaContext = self.llamaContext else {
                    continuation.finish()
                    return
                }
                
                // ì‹¤ì œ LLM ì¶”ë¡ ì€ llama.cpp í†µí•© í›„ êµ¬í˜„
                // ì„ì‹œ ì‹œë®¬ë ˆì´ì…˜
                let responses = [
                    "ì•ˆë…•í•˜ì„¸ìš”! ",
                    "ë¬´ì—‡ì„ ",
                    "ë„ì™€ë“œë¦´ê¹Œìš”? ",
                    "ê¶ê¸ˆí•œ ",
                    "ì ì´ ",
                    "ìˆìœ¼ì‹œë©´ ",
                    "ì–¸ì œë“  ",
                    "ë¬¼ì–´ë³´ì„¸ìš”!"
                ]
                
                for response in responses {
                    try? await Task.sleep(nanoseconds: 100_000_000) // 0.1ì´ˆ
                    continuation.yield(response)
                }
                
                continuation.finish()
            }
        }
    }
}

