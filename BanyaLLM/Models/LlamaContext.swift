//
//  LlamaContext.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

enum LlamaError: Error {
    case couldNotInitializeContext
    case modelNotFound
    case failedToLoadModel
}

// Helper functions for llama_batch
func llama_batch_clear(_ batch: inout llama_batch) {
    batch.n_tokens = 0
}

func llama_batch_add(_ batch: inout llama_batch, _ id: llama_token, _ pos: llama_pos, _ seq_ids: [llama_seq_id], _ logits: Bool) {
    let tokenIndex = Int(batch.n_tokens)
    batch.token[tokenIndex] = id
    batch.pos[tokenIndex] = pos
    batch.n_seq_id[tokenIndex] = Int32(seq_ids.count)
    
    // seq_id ë°°ì—´ì´ nilì´ ì•„ë‹Œì§€ í™•ì¸í•˜ê³  ê°’ í• ë‹¹
    if let seqIdArray = batch.seq_id[tokenIndex] {
        for i in 0..<seq_ids.count {
            seqIdArray[i] = seq_ids[i]
        }
    } else {
        print("âš ï¸ seq_id ë°°ì—´ì´ nilì…ë‹ˆë‹¤. í† í° ì¸ë±ìŠ¤: \(tokenIndex)")
    }
    
    batch.logits[tokenIndex] = logits ? 1 : 0
    batch.n_tokens += 1
}

actor LlamaContext {
    private var model: OpaquePointer?
    private var context: OpaquePointer?
    private var vocab: OpaquePointer?
    private var sampling: UnsafeMutablePointer<llama_sampler>?
    private var batch: llama_batch
    private var tokens_list: [llama_token] = []
    private var temporary_invalid_cchars: [CChar] = []
    
    var isDone: Bool = false
    var n_len: Int32 = 64   // ìµœëŒ€ ìƒì„± í† í° ìˆ˜ (ë§¤ìš° ê°„ê²°í•œ ì‘ë‹µ, 2-3ë¬¸ì¥)
    var n_cur: Int32 = 0
    
    // ê°•ì œ ì¢…ë£Œ ë©”ì„œë“œ
    func forceStop() {
        isDone = true
    }
    var n_decode: Int32 = 0
    
    private let modelPath: String
    
    init(modelPath: String) {
        self.modelPath = modelPath
        // batch í¬ê¸°ë¥¼ 2048ë¡œ ëŠ˜ë ¤ì„œ ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
        self.batch = llama_batch_init(2048, 0, 1)
    }
    
    func initialize() throws {
        guard FileManager.default.fileExists(atPath: modelPath) else {
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: \(modelPath)")
            throw LlamaError.modelNotFound
        }
        
        print("âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸: \(modelPath)")
        print("ğŸ”„ llama.cppë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        // llama.cpp ì´ˆê¸°í™”
        llama_backend_init()
        
        var model_params = llama_model_default_params()
        
        #if targetEnvironment(simulator)
        model_params.n_gpu_layers = 0
        print("ğŸ“± ì‹œë®¬ë ˆì´í„°: CPU ëª¨ë“œ")
        #else
        // GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€: ì¼ë¶€ ë ˆì´ì–´ë§Œ GPUì— ë¡œë“œ
        model_params.n_gpu_layers = 24  // 33ê°œ ì¤‘ 24ê°œë§Œ GPU (ì•½ 70%)
        print("âš¡ ì‹¤ì œ ê¸°ê¸°: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (GPU: 24ë ˆì´ì–´, CPU: 9ë ˆì´ì–´)")
        #endif
        
        guard let loadedModel = llama_model_load_from_file(modelPath, model_params) else {
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            throw LlamaError.couldNotInitializeContext
        }
        self.model = loadedModel
        
        let n_threads = max(1, min(8, ProcessInfo.processInfo.processorCount - 2))
        print("ğŸ§µ ìŠ¤ë ˆë“œ ìˆ˜: \(n_threads)")
        
        var ctx_params = llama_context_default_params()
        ctx_params.n_ctx = 1024  // 2048 â†’ 1024ë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
        ctx_params.n_threads = Int32(n_threads)
        ctx_params.n_threads_batch = Int32(n_threads)
        
        print("ğŸ›ï¸ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: 1024 (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        guard let loadedContext = llama_init_from_model(loadedModel, ctx_params) else {
            print("âŒ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            throw LlamaError.couldNotInitializeContext
        }
        self.context = loadedContext
        
        // Sampling ì´ˆê¸°í™” (Llama 3.1 ìµœì í™”)
        let sparams = llama_sampler_chain_default_params()
        self.sampling = llama_sampler_chain_init(sparams)
        
        // 1. Top-K ìƒ˜í”Œë§ (0 = ë¹„í™œì„±í™”, Llama 3.1 ê¶Œì¥)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_top_k(0))
        
        // 2. Top-P (Nucleus Sampling) - 0.9
        llama_sampler_chain_add(self.sampling, llama_sampler_init_top_p(0.9, 1))
        
        // 3. Min-P - ë‚®ì€ í™•ë¥  í† í° ë°°ì œ (Llama 3.1 í•µì‹¬ ì„¤ì •)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_min_p(0.05, 1))
        
        // 4. Temperature - ì°½ì˜ì„± ì¡°ì ˆ (0.6 = ë” ê²°ì •ë¡ ì , ë°˜ë³µ ê°ì†Œ)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_temp(0.6))
        
        // 5. Repeat Penalty - ë°˜ë³µ ë°©ì§€ ê°•í™” (1.15 = ê°•í•œ íŒ¨ë„í‹°, last_n=64 = ìµœê·¼ 64 í† í°ë§Œ ê³ ë ¤)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_penalties(
            64,     // last_n: ìµœê·¼ 64 í† í°ë§Œ ê³ ë ¤ (ë°˜ë³µ ê°ì§€ ì •í™•ë„ í–¥ìƒ)
            1.15,   // repeat_penalty: ê°•í•œ ë°˜ë³µ íŒ¨ë„í‹° (1.05 â†’ 1.15)
            0.1,    // freq_penalty: ë¹ˆë„ íŒ¨ë„í‹° ì¶”ê°€ (ë°˜ë³µ ë‹¨ì–´ ì–µì œ)
            0.1     // presence_penalty: ì¡´ì¬ íŒ¨ë„í‹° ì¶”ê°€ (ì´ë¯¸ ë‚˜ì˜¨ ë‹¨ì–´ ì–µì œ)
        ))
        
        // 6. Dist ìƒ˜í”Œë§ (ìµœì¢… í† í° ì„ íƒ)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_dist(UInt32.random(in: 0...1000)))
        
        print("ğŸ›ï¸ ìƒ˜í”Œë§ ì„¤ì •: Temp=0.6, Top-P=0.9, Min-P=0.05, Repeat=1.15 (last_n=64), Freq=0.1, Presence=0.1")
        
        self.vocab = llama_model_get_vocab(loadedModel)
        
        print("âœ… llama.cpp ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    }
    
    func completionInit(text: String) {
        print("ğŸš€ ì¶”ë¡  ì‹œì‘")
        
        guard let context = context else { 
            print("âŒ contextê°€ nilì…ë‹ˆë‹¤!")
            return 
        }
        
        tokens_list = tokenize(text: text, add_bos: true)
        temporary_invalid_cchars = []
        
        print("ğŸ”¢ í† í°í™”: \(tokens_list.count)ê°œ")
        
        let n_ctx = llama_n_ctx(context)
        let n_kv_req = tokens_list.count + (Int(n_len) - tokens_list.count)
        
        
        if n_kv_req > n_ctx {
            print("âš ï¸ ê²½ê³ : n_kv_req > n_ctx")
        }
        
        llama_batch_clear(&batch)
        
        // batch í¬ê¸° ì œí•œ í™•ì¸ (2048)
        let maxBatchSize = 2048
        if tokens_list.count > maxBatchSize {
            print("âš ï¸ ê²½ê³ : í† í° ìˆ˜(\(tokens_list.count))ê°€ batch í¬ê¸°(\(maxBatchSize))ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì²˜ìŒ \(maxBatchSize)ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        }
        
        // batchì— í† í° ì¶”ê°€ (ìµœëŒ€ batch í¬ê¸°ê¹Œì§€ë§Œ)
        let tokensToAdd = min(tokens_list.count, maxBatchSize)
        for i in 0..<tokensToAdd {
            // seq_id ë°°ì—´ nil ì²´í¬
            let seqIdArray = batch.seq_id[Int(batch.n_tokens)]
            if seqIdArray != nil {
                llama_batch_add(&batch, tokens_list[i], Int32(i), [0], false)
            } else {
                print("âš ï¸ seq_id ë°°ì—´ì´ nilì…ë‹ˆë‹¤. í† í° ì¸ë±ìŠ¤: \(i) - batch í¬ê¸° ì´ˆê³¼ ê°€ëŠ¥ì„±")
                break
            }
        }
        
        if batch.n_tokens > 0 {
            batch.logits[Int(batch.n_tokens) - 1] = 1
            
            if llama_decode(context, batch) != 0 {
                print("âŒ llama_decode() ì‹¤íŒ¨")
            }
        } else {
            print("âŒ batchì— í† í°ì´ ì—†ìŠµë‹ˆë‹¤!")
        }
        
        n_cur = batch.n_tokens
        isDone = false
    }
    
    func completionLoop() -> String {
        guard let context = context,
              let sampling = sampling,
              let vocab = vocab else {
            isDone = true
            return ""
        }
        
        let new_token_id = llama_sampler_sample(sampling, context, batch.n_tokens - 1)
        
        // EOG í† í° ê°ì§€ (Llama 3.1 EOG í† í° ID ì§ì ‘ ë¹„êµ)
        // 128001: <|end_of_text|>, 128008: <|eom_id|>, 128009: <|eot_id|>
        let isEOG = (new_token_id == 128001 || new_token_id == 128008 || new_token_id == 128009)
        
        if isEOG || n_cur == n_len {
            print("âœ… ìƒì„± ì™„ë£Œ (EOG: \(isEOG), í† í°: \(n_cur)ê°œ)")
            isDone = true
            temporary_invalid_cchars.removeAll()
            return "" // EOG í† í°ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        }
        
        // íŠ¹ìˆ˜ í† í° í•„í„°ë§ (Llama 3.1 íŠ¹ìˆ˜ í† í°ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
        // 128000-128255: ëª¨ë“  íŠ¹ìˆ˜ í† í° ë²”ìœ„
        if new_token_id >= 128000 {
            // íŠ¹ìˆ˜ í† í°ì€ ë°°ì¹˜ì— ì¶”ê°€í•˜ì§€ë§Œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
            llama_batch_clear(&batch)
            llama_batch_add(&batch, new_token_id, n_cur, [0], true)
            
            n_decode += 1
            n_cur += 1
            
            if llama_decode(context, batch) != 0 {
                print("âŒ llama_decode ì‹¤íŒ¨!")
            }
            
            return "" // ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (íŠ¹ìˆ˜ í† í°ì€ ì¶œë ¥ ì•ˆ í•¨)
        }
        
        let new_token_cchars = token_to_piece(token: new_token_id)
        temporary_invalid_cchars.append(contentsOf: new_token_cchars)
        var new_token_str: String
        if let string = String(validatingUTF8: temporary_invalid_cchars + [0]) {
            temporary_invalid_cchars.removeAll()
            new_token_str = string
        } else if (0..<temporary_invalid_cchars.count).contains(where: {
            $0 != 0 && String(validatingUTF8: Array(temporary_invalid_cchars.suffix($0)) + [0]) != nil
        }) {
            let string = String(cString: temporary_invalid_cchars + [0])
            temporary_invalid_cchars.removeAll()
            new_token_str = string
        } else {
            new_token_str = ""
        }
        
        // Llama 3.1 íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ í•„í„°ë§
        // ëª¨ë¸ì´ ì¼ë°˜ í† í°ìœ¼ë¡œ íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ
        let specialTokenPatterns = [
            "<|begin_of_text|>",
            "<|end_of_text|>",
            "<|start_header_id|>",
            "<|end_header_id|>",
            "<|eot_id|>",
            "<|eom_id|>",
            "<|python_tag|>",
            "<|finetune_right_pad_id|>"
        ]
        
        for pattern in specialTokenPatterns {
            new_token_str = new_token_str.replacingOccurrences(of: pattern, with: "")
        }
        
        // reserved_special_token íŒ¨í„´ ì œê±° (ì •ê·œì‹ ì‚¬ìš©)
        if let regex = try? NSRegularExpression(pattern: "<\\|reserved_special_token_\\d+\\|>", options: []) {
            let range = NSRange(new_token_str.startIndex..., in: new_token_str)
            new_token_str = regex.stringByReplacingMatches(
                in: new_token_str,
                options: [],
                range: range,
                withTemplate: ""
            )
        }
        
        // ë¶€ë¶„ íŠ¹ìˆ˜ í† í° íŒ¨í„´ í•„í„°ë§ (í† í°ì´ ë¶„í•´ë˜ì–´ ìƒì„±ë˜ëŠ” ê²½ìš°)
        // ì˜ˆ: '<|', '|>', ë‹¨ë… '|' ë“±
        let partialPatterns = [
            "<|",  // íŠ¹ìˆ˜ í† í° ì‹œì‘
            "|>",  // íŠ¹ìˆ˜ í† í° ë
            "^\\|$",  // ë‹¨ë… íŒŒì´í”„ (ì •ê·œì‹)
            "^<\\|",  // '<|'ë¡œ ì‹œì‘
            "\\|>$"   // '|>'ë¡œ ë
        ]
        
        // ë‹¨ë… íŒŒì´í”„ ì œê±°
        if new_token_str == "|" {
            new_token_str = ""
        }
        
        // '<|' ë˜ëŠ” '|>' í¬í•¨ ì‹œ ì œê±°
        if new_token_str.contains("<|") || new_token_str.contains("|>") {
            new_token_str = new_token_str.replacingOccurrences(of: "<|", with: "")
            new_token_str = new_token_str.replacingOccurrences(of: "|>", with: "")
        }
        
        // ì •ê·œì‹ìœ¼ë¡œ ë¶€ë¶„ íŒ¨í„´ ì œê±°
        if let regex = try? NSRegularExpression(pattern: "<\\|.*?\\|>", options: []) {
            let range = NSRange(new_token_str.startIndex..., in: new_token_str)
            new_token_str = regex.stringByReplacingMatches(
                in: new_token_str,
                options: [],
                range: range,
                withTemplate: ""
            )
        }
        
        llama_batch_clear(&batch)
        llama_batch_add(&batch, new_token_id, n_cur, [0], true)
        
        n_decode += 1
        n_cur += 1
        
        if llama_decode(context, batch) != 0 {
            print("âŒ llama_decode ì‹¤íŒ¨!")
        }
        
        // ìƒì„±ëœ í† í° ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        if !new_token_str.isEmpty {
            print("ğŸ”¤ í† í° ì¶œë ¥: '\(new_token_str)' (ID: \(new_token_id))")
        }
        
        return new_token_str
    }
    
    func clear() {
        guard let context = context else { return }
        
        tokens_list.removeAll()
        temporary_invalid_cchars.removeAll()
        llama_memory_clear(llama_get_memory(context), true)
        n_cur = 0
        n_decode = 0
        isDone = false
    }
    
    func modelInfo() -> String {
        guard let model = model else {
            return "ëª¨ë¸ ë¯¸ë¡œë“œ"
        }
        
        let result = UnsafeMutablePointer<Int8>.allocate(capacity: 256)
        result.initialize(repeating: Int8(0), count: 256)
        defer {
            result.deallocate()
        }
        
        let nChars = llama_model_desc(model, result, 256)
        let bufferPointer = UnsafeBufferPointer(start: result, count: Int(nChars))
        
        var swiftString = ""
        for char in bufferPointer {
            swiftString.append(Character(UnicodeScalar(UInt8(char))))
        }
        
        return swiftString
    }
    
    private func tokenize(text: String, add_bos: Bool) -> [llama_token] {
        guard let vocab = vocab else { return [] }
        
        let utf8Count = text.utf8.count
        let n_tokens = utf8Count + (add_bos ? 1 : 0) + 1
        let tokens = UnsafeMutablePointer<llama_token>.allocate(capacity: n_tokens)
        let tokenCount = llama_tokenize(vocab, text, Int32(utf8Count), tokens, Int32(n_tokens), add_bos, false)
        
        var swiftTokens: [llama_token] = []
        for i in 0..<tokenCount {
            swiftTokens.append(tokens[Int(i)])
        }
        
        tokens.deallocate()
        
        return swiftTokens
    }
    
    private func token_to_piece(token: llama_token) -> [CChar] {
        guard let vocab = vocab else { return [] }
        
        let result = UnsafeMutablePointer<Int8>.allocate(capacity: 8)
        result.initialize(repeating: Int8(0), count: 8)
        defer {
            result.deallocate()
        }
        let nTokens = llama_token_to_piece(vocab, token, result, 8, 0, false)
        
        if nTokens < 0 {
            let newResult = UnsafeMutablePointer<Int8>.allocate(capacity: Int(-nTokens))
            newResult.initialize(repeating: Int8(0), count: Int(-nTokens))
            defer {
                newResult.deallocate()
            }
            let nNewTokens = llama_token_to_piece(vocab, token, newResult, -nTokens, 0, false)
            let bufferPointer = UnsafeBufferPointer(start: newResult, count: Int(nNewTokens))
            return Array(bufferPointer)
        } else {
            let bufferPointer = UnsafeBufferPointer(start: result, count: Int(nTokens))
            return Array(bufferPointer)
        }
    }
    
    deinit {
        if let sampling = sampling {
            llama_sampler_free(sampling)
        }
        llama_batch_free(batch)
        if let model = model {
            llama_model_free(model)
        }
        if let context = context {
            llama_free(context)
        }
        llama_backend_free()
    }
}
