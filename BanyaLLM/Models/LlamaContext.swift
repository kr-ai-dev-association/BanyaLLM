//
//  LlamaContext.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//
//  NOTE: llama.cpp í†µí•©ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
//  í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•˜ë©°, ì‹¤ì œ llama.cppë¥¼ í†µí•©í•˜ë ¤ë©´:
//  1. llama.cppë¥¼ ë¹Œë“œí•˜ì—¬ xcframework ìƒì„±
//  2. í”„ë¡œì íŠ¸ì— xcframework ì¶”ê°€
//  3. ì•„ë˜ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œ í™œì„±í™”
//

import Foundation

enum LlamaError: Error {
    case couldNotInitializeContext
    case modelNotFound
    case failedToLoadModel
}

actor LlamaContext {
    private var modelPath: String
    private var isInitialized: Bool = false
    private var tokens_list: [String] = []
    
    var isDone: Bool = false
    var n_len: Int32 = 512  // ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    var n_cur: Int32 = 0    // í˜„ì¬ í† í° ìœ„ì¹˜
    
    init(modelPath: String) {
        self.modelPath = modelPath
    }
    
    func initialize() throws {
        guard FileManager.default.fileExists(atPath: modelPath) else {
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: \(modelPath)")
            throw LlamaError.modelNotFound
        }
        
        print("âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸: \(modelPath)")
        print("ğŸ“ llama.cpp í†µí•© ì‹œ ì—¬ê¸°ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤")
        
        // TODO: llama.cpp í†µí•© ì‹œ ì•„ë˜ ì½”ë“œ í™œì„±í™”
        /*
        llama_backend_init()
        var model_params = llama_model_default_params()
        
        #if targetEnvironment(simulator)
        model_params.n_gpu_layers = 0
        #else
        model_params.n_gpu_layers = 999  // MPS ê°€ì† ì‚¬ìš©
        #endif
        
        let model = llama_model_load_from_file(modelPath, model_params)
        guard let model else {
            throw LlamaError.couldNotInitializeContext
        }
        
        let n_threads = max(1, min(8, ProcessInfo.processInfo.processorCount - 2))
        var ctx_params = llama_context_default_params()
        ctx_params.n_ctx = 2048
        ctx_params.n_threads = Int32(n_threads)
        ctx_params.n_threads_batch = Int32(n_threads)
        
        let context = llama_init_from_model(model, ctx_params)
        guard let context else {
            throw LlamaError.couldNotInitializeContext
        }
        */
        
        isInitialized = true
    }
    
    func completionInit(text: String) {
        print("ğŸš€ ì¶”ë¡  ì‹œì‘: \(text)")
        isDone = false
        n_cur = 0
        tokens_list = []
        
        // TODO: llama.cpp í†µí•© ì‹œ í† í°í™” ë° ë°°ì¹˜ ì²˜ë¦¬
    }
    
    func completionLoop() -> String {
        // ì‹œë®¬ë ˆì´ì…˜: í•œêµ­ì–´ ì‘ë‹µ ìƒì„±
        let tokens = [
            "ì•ˆë…•í•˜ì„¸ìš”", "!", " ì €ëŠ”", " BanyaLLM", "ì…ë‹ˆë‹¤", ".",
            " ë¬´ì—‡ì„", " ë„ì™€", "ë“œë¦´ê¹Œìš”", "?", " ê¶ê¸ˆí•œ", " ì ì´",
            " ìˆìœ¼ì‹œë©´", " ì–¸ì œë“ ", " ë§ì”€í•´", "ì£¼ì„¸ìš”", "!"
        ]
        
        if n_cur < tokens.count {
            let token = tokens[Int(n_cur)]
            n_cur += 1
            return token
        } else {
            isDone = true
            return ""
        }
        
        // TODO: llama.cpp í†µí•© ì‹œ ì•„ë˜ ì½”ë“œ í™œì„±í™”
        /*
        let new_token_id = llama_sampler_sample(sampling, context, batch.n_tokens - 1)
        
        if llama_vocab_is_eog(vocab, new_token_id) || n_cur == n_len {
            isDone = true
            return ""
        }
        
        let new_token_str = token_to_string(token: new_token_id)
        n_cur += 1
        
        llama_batch_clear(&batch)
        llama_batch_add(&batch, new_token_id, n_cur, [0], true)
        
        if llama_decode(context, batch) != 0 {
            print("Failed to decode")
        }
        
        return new_token_str
        */
    }
    
    func clear() {
        tokens_list.removeAll()
        n_cur = 0
        isDone = false
        
        // TODO: llama.cpp í†µí•© ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
        // llama_memory_clear(llama_get_memory(context), true)
    }
    
    func modelInfo() -> String {
        return "BanyaLLM (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ)\nëª¨ë¸: llama31-banyaa-q4_k_m.gguf\ní¬ê¸°: 4.6GB\nìƒíƒœ: llama.cpp í†µí•© í•„ìš”"
    }
    
    deinit {
        // TODO: llama.cpp í†µí•© ì‹œ ë¦¬ì†ŒìŠ¤ í•´ì œ
        /*
        llama_sampler_free(sampling)
        llama_batch_free(batch)
        llama_model_free(model)
        llama_free(context)
        llama_backend_free()
        */
    }
}

