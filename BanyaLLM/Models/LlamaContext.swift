//
//  LlamaContext.swift
//  BanyaLLM
//
//  Created by Tony-M4 on 11/13/25.
//

import Foundation

enum LlamaError: Error {
    case couldNotInitializeContext
    case modelNotFound
    case failedToLoadModel
}

// Helper functions for llama_batch
func llama_batch_clear(_ batch: inout llama_batch) {
    batch.n_tokens = 0
}

func llama_batch_add(_ batch: inout llama_batch, _ id: llama_token, _ pos: llama_pos, _ seq_ids: [llama_seq_id], _ logits: Bool) {
    batch.token   [Int(batch.n_tokens)] = id
    batch.pos     [Int(batch.n_tokens)] = pos
    batch.n_seq_id[Int(batch.n_tokens)] = Int32(seq_ids.count)
    for i in 0..<seq_ids.count {
        batch.seq_id[Int(batch.n_tokens)]![Int(i)] = seq_ids[i]
    }
    batch.logits  [Int(batch.n_tokens)] = logits ? 1 : 0
    
    batch.n_tokens += 1
}

actor LlamaContext {
    private var model: OpaquePointer?
    private var context: OpaquePointer?
    private var vocab: OpaquePointer?
    private var sampling: UnsafeMutablePointer<llama_sampler>?
    private var batch: llama_batch
    private var tokens_list: [llama_token] = []
    private var temporary_invalid_cchars: [CChar] = []
    
    var isDone: Bool = false
    var n_len: Int32 = 512
    var n_cur: Int32 = 0
    var n_decode: Int32 = 0
    
    private let modelPath: String
    
    init(modelPath: String) {
        self.modelPath = modelPath
        self.batch = llama_batch_init(512, 0, 1)
    }
    
    func initialize() throws {
        guard FileManager.default.fileExists(atPath: modelPath) else {
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: \(modelPath)")
            throw LlamaError.modelNotFound
        }
        
        print("âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸: \(modelPath)")
        print("ğŸ”„ llama.cppë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        // llama.cpp ì´ˆê¸°í™”
        llama_backend_init()
        
        var model_params = llama_model_default_params()
        
        #if targetEnvironment(simulator)
        model_params.n_gpu_layers = 0
        print("ğŸ“± ì‹œë®¬ë ˆì´í„°: CPU ëª¨ë“œ")
        #else
        // GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€: ì¼ë¶€ ë ˆì´ì–´ë§Œ GPUì— ë¡œë“œ
        model_params.n_gpu_layers = 24  // 33ê°œ ì¤‘ 24ê°œë§Œ GPU (ì•½ 70%)
        print("âš¡ ì‹¤ì œ ê¸°ê¸°: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (GPU: 24ë ˆì´ì–´, CPU: 9ë ˆì´ì–´)")
        #endif
        
        guard let loadedModel = llama_model_load_from_file(modelPath, model_params) else {
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            throw LlamaError.couldNotInitializeContext
        }
        self.model = loadedModel
        
        let n_threads = max(1, min(8, ProcessInfo.processInfo.processorCount - 2))
        print("ğŸ§µ ìŠ¤ë ˆë“œ ìˆ˜: \(n_threads)")
        
        var ctx_params = llama_context_default_params()
        ctx_params.n_ctx = 1024  // 2048 â†’ 1024ë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
        ctx_params.n_threads = Int32(n_threads)
        ctx_params.n_threads_batch = Int32(n_threads)
        
        print("ğŸ›ï¸ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: 1024 (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        guard let loadedContext = llama_init_from_model(loadedModel, ctx_params) else {
            print("âŒ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            throw LlamaError.couldNotInitializeContext
        }
        self.context = loadedContext
        
        // Sampling ì´ˆê¸°í™” (Llama 3.1 ìµœì í™”)
        let sparams = llama_sampler_chain_default_params()
        self.sampling = llama_sampler_chain_init(sparams)
        
        // 1. Top-K ìƒ˜í”Œë§ (0 = ë¹„í™œì„±í™”, Llama 3.1 ê¶Œì¥)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_top_k(0))
        
        // 2. Top-P (Nucleus Sampling) - 0.9
        llama_sampler_chain_add(self.sampling, llama_sampler_init_top_p(0.9, 1))
        
        // 3. Min-P - ë‚®ì€ í™•ë¥  í† í° ë°°ì œ (Llama 3.1 í•µì‹¬ ì„¤ì •)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_min_p(0.05, 1))
        
        // 4. Temperature - ì°½ì˜ì„± ì¡°ì ˆ (0.7 = ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_temp(0.7))
        
        // 5. Repeat Penalty - ë°˜ë³µ ë°©ì§€ (1.05 = ì ë‹¹í•œ íŒ¨ë„í‹°)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_penalties(
            512,    // last_n: ìµœê·¼ 512 í† í° ê³ ë ¤
            1.05,   // repeat_penalty: ë°˜ë³µ íŒ¨ë„í‹°
            0.0,    // freq_penalty
            0.0     // presence_penalty
        ))
        
        // 6. Dist ìƒ˜í”Œë§ (ìµœì¢… í† í° ì„ íƒ)
        llama_sampler_chain_add(self.sampling, llama_sampler_init_dist(UInt32.random(in: 0...1000)))
        
        print("ğŸ›ï¸ ìƒ˜í”Œë§ ì„¤ì •: Temp=0.7, Top-P=0.9, Min-P=0.05, Repeat=1.05")
        
        self.vocab = llama_model_get_vocab(loadedModel)
        
        print("âœ… llama.cpp ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    }
    
    func completionInit(text: String) {
        print("ğŸš€ ì¶”ë¡  ì‹œì‘")
        
        guard let context = context else { 
            print("âŒ contextê°€ nilì…ë‹ˆë‹¤!")
            return 
        }
        
        tokens_list = tokenize(text: text, add_bos: true)
        temporary_invalid_cchars = []
        
        print("ğŸ”¢ í† í°í™”: \(tokens_list.count)ê°œ")
        
        let n_ctx = llama_n_ctx(context)
        let n_kv_req = tokens_list.count + (Int(n_len) - tokens_list.count)
        
        
        if n_kv_req > n_ctx {
            print("âš ï¸ ê²½ê³ : n_kv_req > n_ctx")
        }
        
        llama_batch_clear(&batch)
        
        for i in 0..<tokens_list.count {
            llama_batch_add(&batch, tokens_list[i], Int32(i), [0], false)
        }
        batch.logits[Int(batch.n_tokens) - 1] = 1
        
        if llama_decode(context, batch) != 0 {
            print("âŒ llama_decode() ì‹¤íŒ¨")
        }
        
        n_cur = batch.n_tokens
        isDone = false
    }
    
    func completionLoop() -> String {
        print("ğŸ” completionLoop ì§„ì…")
        
        guard let context = context,
              let sampling = sampling,
              let vocab = vocab else {
            print("âŒ context/sampling/vocab ì¤‘ nil ë°œê²¬")
            isDone = true
            return ""
        }
        
        print("ğŸ² ìƒ˜í”Œë§ ì‹œì‘ (batch.n_tokens: \(batch.n_tokens))")
        let new_token_id = llama_sampler_sample(sampling, context, batch.n_tokens - 1)
        print("ğŸ² ìƒ˜í”Œë§ ì™„ë£Œ: í† í° ID = \(new_token_id)")
        
        // EOG í† í° ê°ì§€ (llama_token_is_eog ì‚¬ìš©)
        print("ğŸ” EOG ê°ì§€ ì‹œì‘...")
        guard let model = model else {
            print("âŒ modelì´ nil")
            isDone = true
            return ""
        }
        
        print("ğŸ” llama_token_is_eog í˜¸ì¶œ ì¤‘...")
        let isEOG = llama_token_is_eog(model, new_token_id)
        print("ğŸ” llama_token_is_eog ì™„ë£Œ: \(isEOG)")
        
        if isEOG || n_cur == n_len {
            print("âœ… ìƒì„± ì™„ë£Œ (EOG: \(isEOG), í† í°: \(n_cur)ê°œ)")
            isDone = true
            let new_token_str = String(cString: temporary_invalid_cchars + [0])
            temporary_invalid_cchars.removeAll()
            return new_token_str
        }
        
        print("ğŸ”¤ token_to_piece í˜¸ì¶œ ì¤‘...")
        let new_token_cchars = token_to_piece(token: new_token_id)
        print("ğŸ”¤ token_to_piece ì™„ë£Œ: \(new_token_cchars.count)ë°”ì´íŠ¸")
        temporary_invalid_cchars.append(contentsOf: new_token_cchars)
        let new_token_str: String
        if let string = String(validatingUTF8: temporary_invalid_cchars + [0]) {
            temporary_invalid_cchars.removeAll()
            new_token_str = string
        } else if (0..<temporary_invalid_cchars.count).contains(where: {
            $0 != 0 && String(validatingUTF8: Array(temporary_invalid_cchars.suffix($0)) + [0]) != nil
        }) {
            let string = String(cString: temporary_invalid_cchars + [0])
            temporary_invalid_cchars.removeAll()
            new_token_str = string
        } else {
            new_token_str = ""
            print("â³ UTF8 ëŒ€ê¸° ì¤‘...")
        }
        
        print("ğŸ”„ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘...")
        llama_batch_clear(&batch)
        llama_batch_add(&batch, new_token_id, n_cur, [0], true)
        print("ğŸ”„ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        n_decode += 1
        n_cur += 1
        
        print("ğŸ”„ llama_decode í˜¸ì¶œ ì¤‘...")
        if llama_decode(context, batch) != 0 {
            print("âŒ llama_decode ì‹¤íŒ¨!")
        } else {
            print("âœ… llama_decode ì„±ê³µ")
        }
        
        print("ğŸ í† í° ë°˜í™˜: '\(new_token_str)'")
        return new_token_str
    }
    
    func clear() {
        guard let context = context else { return }
        
        tokens_list.removeAll()
        temporary_invalid_cchars.removeAll()
        llama_memory_clear(llama_get_memory(context), true)
        n_cur = 0
        n_decode = 0
        isDone = false
    }
    
    func modelInfo() -> String {
        guard let model = model else {
            return "ëª¨ë¸ ë¯¸ë¡œë“œ"
        }
        
        let result = UnsafeMutablePointer<Int8>.allocate(capacity: 256)
        result.initialize(repeating: Int8(0), count: 256)
        defer {
            result.deallocate()
        }
        
        let nChars = llama_model_desc(model, result, 256)
        let bufferPointer = UnsafeBufferPointer(start: result, count: Int(nChars))
        
        var swiftString = ""
        for char in bufferPointer {
            swiftString.append(Character(UnicodeScalar(UInt8(char))))
        }
        
        return swiftString
    }
    
    private func tokenize(text: String, add_bos: Bool) -> [llama_token] {
        guard let vocab = vocab else { return [] }
        
        let utf8Count = text.utf8.count
        let n_tokens = utf8Count + (add_bos ? 1 : 0) + 1
        let tokens = UnsafeMutablePointer<llama_token>.allocate(capacity: n_tokens)
        let tokenCount = llama_tokenize(vocab, text, Int32(utf8Count), tokens, Int32(n_tokens), add_bos, false)
        
        var swiftTokens: [llama_token] = []
        for i in 0..<tokenCount {
            swiftTokens.append(tokens[Int(i)])
        }
        
        tokens.deallocate()
        
        return swiftTokens
    }
    
    private func token_to_piece(token: llama_token) -> [CChar] {
        guard let vocab = vocab else { return [] }
        
        let result = UnsafeMutablePointer<Int8>.allocate(capacity: 8)
        result.initialize(repeating: Int8(0), count: 8)
        defer {
            result.deallocate()
        }
        let nTokens = llama_token_to_piece(vocab, token, result, 8, 0, false)
        
        if nTokens < 0 {
            let newResult = UnsafeMutablePointer<Int8>.allocate(capacity: Int(-nTokens))
            newResult.initialize(repeating: Int8(0), count: Int(-nTokens))
            defer {
                newResult.deallocate()
            }
            let nNewTokens = llama_token_to_piece(vocab, token, newResult, -nTokens, 0, false)
            let bufferPointer = UnsafeBufferPointer(start: newResult, count: Int(nNewTokens))
            return Array(bufferPointer)
        } else {
            let bufferPointer = UnsafeBufferPointer(start: result, count: Int(nTokens))
            return Array(bufferPointer)
        }
    }
    
    deinit {
        if let sampling = sampling {
            llama_sampler_free(sampling)
        }
        llama_batch_free(batch)
        if let model = model {
            llama_model_free(model)
        }
        if let context = context {
            llama_free(context)
        }
        llama_backend_free()
    }
}
